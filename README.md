# LLMPromptAttackGuide

# 大语言模型 Prompt 攻击手册

这个一份关于大语言模型攻击的实战手册，SecureNexusLab LLM团队通过文献研究、测试床搭建、开源模型测试等一系列工作并完成此手册。得益于 OpenAI 在近些年的高速增长，生成式 AI 及其相关应用吸引力无数用户和投资者的目光，GenAI 集成的软件正在以一种磅礴的生命力高速成长着。伴随着惊人的增长速度的也是持续增长的风险和待解决漏洞。基于大语言模型现在不能清晰地识别数据和指令区别的弱点，许多攻击者利用诸如提示注入、角色扮演等手段从大语言模型中获取本应该被拦截的信息。尽管市面上对众多新出现的威胁并没有有效的缓解措施，但是通过对于攻击方式进行分类总结与归纳，提供对漏洞的认识，可以促进从业者部署安全可靠的强大模型，并提供例如语义近似匹配工具等防御措施来保护用于免受潜在攻击的威胁。

本手册旨在帮助安全行业从业者及爱好者快速了解 LLM Prompt 攻击定义和原理，掌握 LLM Prompt 攻击方式，并掌握分析 LLM 漏洞和深入研究的能力。理解 LLM 攻击不需要精通代码编写和工程实践，只需要你足够耐心的阅读和自我实践。与此同时，包括角色扮演等在内的诸多攻击手段只有用于非法目的时才违法，通过对于不同 Prompt 的学习，读者也可以建立起对于 GenAI 应用的认知以及深入了解如何能够更好地使用大语言模型帮助我们的工作与生活。

您通过本链接 [大语言模型 Prompt 攻击手册](https://github.com/SecureNexusLab/LLMPromptAttackGuide/blob/main/SecureNexusLab_LLM_Attack.pdf) 获取手册的 pdf 文件。

## 分发许可 License

本手册使用 MIT License

## 贡献者 Contributors

感谢各师傅的辛勤付出，在阅读和学习的过程中有遇到描述不妥或错误、或有对本手册遗漏知识的补充，亦或者在学习过程中需要帮助，您都可以通过以下联系方式联系我们，我们诚挚欢迎各位师傅的加入！

:handshake: TheBinKing:e-mail:thebinking66@gmail.com

:handshake: Yuchen Fang (@Anxiu0101):e-mail:anxiu.fyc@foxmail.com

:handshake: littlebird :e-mail:mcc531640@gmail.com


> 如果本教程对您有所帮助，我们深感荣幸，如在阅读中您有新内容补充或新想法提供，欢迎与我们一同构建~
> 此外欢迎各位师傅加入SecureNexusLab一起构建一个高质量的安全社群，一起讨论前沿的技术~（外部交流群：QQ3群：701604947，QQ4群：701934709）
